# ✅ 实时自动爬取 - 配置完成！

## 🎉 功能已启用

您的风险预警平台现在**每小时自动爬取**所有数据源，无需手动操作！

---

## 📊 当前配置

### ✅ 已完成设置

| 项目 | 状态 | 说明 |
|------|------|------|
| 主服务 | ✅ 运行中 | risk-alert-platform |
| 定时任务 | ✅ 已配置 | auto-crawler |
| 执行频率 | ✅ 每小时 | 每小时的第0分钟 |
| 日志记录 | ✅ 已启用 | logs/auto_crawler.log |

---

## ⏰ 执行计划

### 下次执行时间

定时任务将在**每小时的第0分钟**自动执行：

```
07:00:00 ← 下次执行
08:00:00
09:00:00
10:00:00
...
```

### 执行内容

每次执行时自动：
1. 调用 `POST /api/crawl/all`
2. 爬取所有12个RSS数据源
3. 分析新闻内容识别风险
4. 保存新风险到数据库
5. 记录执行日志

---

## 📝 查看日志

### 实时监控

```bash
# 实时查看爬取日志
tail -f /home/user/webapp/logs/auto_crawler.log

# 查看最近日志
cat /home/user/webapp/logs/auto_crawler.log

# 查看PM2日志
pm2 logs --nostream
```

### 日志示例

```
[2026-01-04 07:00:00] ========================================
[2026-01-04 07:00:00] 开始自动爬取任务
[2026-01-04 07:00:15] 爬取完成: 成功=6, 失败=4, 新增风险=12
[2026-01-04 07:00:15] 自动爬取任务执行成功

[2026-01-04 08:00:00] ========================================
[2026-01-04 08:00:00] 开始自动爬取任务
[2026-01-04 08:00:18] 爬取完成: 成功=7, 失败=3, 新增风险=8
[2026-01-04 08:00:18] 自动爬取任务执行成功
```

---

## 🔧 管理命令

### 查看状态

```bash
cd /home/user/webapp

# 查看PM2进程列表
pm2 list

# 查看定时任务信息
pm2 info auto-crawler
```

### 手动触发（不等待定时）

```bash
cd /home/user/webapp

# 立即执行一次爬取
node auto_crawler.cjs

# 或直接调用API
curl -X POST http://localhost:3000/api/crawl/all
```

### 修改执行频率

编辑 `ecosystem.config.cjs` 文件：

```bash
cd /home/user/webapp
vim ecosystem.config.cjs

# 找到 cron_restart 行，修改为：
# 每30分钟: '*/30 * * * *'
# 每15分钟: '*/15 * * * *'
# 每天2点: '0 2 * * *'

# 然后重启
pm2 restart all
```

### 停止/启动定时任务

```bash
# 停止定时任务（保留主服务）
pm2 stop auto-crawler

# 恢复定时任务
pm2 start auto-crawler

# 重启所有服务
pm2 restart all
```

---

## 📈 预期效果

### 每小时数据

| 指标 | 预期值 |
|------|--------|
| 爬取数据源 | 12个 |
| 成功率 | 60-80% |
| 新增风险 | 5-20条 |
| 执行时间 | 5-15秒 |
| CPU占用 | < 5% |
| 内存占用 | < 50MB |

### 每日累计

- **总爬取次数**：24次
- **新增风险数**：120-480条
- **数据覆盖**：8家海外子公司
- **监控国家**：8个国家

---

## 🎯 验证清单

请在接下来的1小时内验证：

- [  ] 等待到下一个小时的第0分钟（例如07:00）
- [ ] 检查 `logs/auto_crawler.log` 有新日志
- [ ] 访问主页查看新增风险数据
- [ ] 查看统计数据验证数据更新

### 快速验证

```bash
# 1. 手动执行一次测试
cd /home/user/webapp && node auto_crawler.cjs

# 2. 查看日志
cat logs/auto_crawler.log

# 3. 查看最新风险
curl "http://localhost:3000/api/risks?page=1&limit=5" | python3 -m json.tool | head -50

# 4. 查看统计
curl "http://localhost:3000/api/statistics" | python3 -m json.tool
```

---

## 💡 最佳实践

### 1. 监控日志

定期检查日志确保正常运行：
```bash
# 每天检查一次
cat /home/user/webapp/logs/auto_crawler.log | grep "爬取完成"
```

### 2. 调整频率

根据需求调整：
- **正常监控**：每小时1次（推荐）
- **重点关注**：每30分钟1次
- **紧急响应**：每15分钟1次

### 3. 性能优化

如果发现性能问题：
- 减少执行频率
- 优化数据源配置
- 增加超时时间

---

## 🔗 相关文档

- **AUTO_CRAWL_GUIDE.md** - 详细配置指南
- **PUBLIC_ACCESS.md** - 公网访问说明
- **README.md** - 项目总览

---

## 📞 故障排查

### Q1: 日志没有更新？

**检查**：
```bash
# 1. 确认PM2运行
pm2 list

# 2. 手动测试
node /home/user/webapp/auto_crawler.cjs

# 3. 查看错误日志
pm2 logs auto-crawler --err
```

### Q2: 爬取失败率高？

**原因**：
- RSS源不可用
- 网络问题
- API超时

**解决**：
- 检查数据源状态
- 增加超时时间
- 使用更可靠的RSS源

### Q3: 如何查看下次执行时间？

**方法**：
```bash
pm2 info auto-crawler | grep "cron"
```

下次执行时间是：**下一个小时的第0分钟**

---

## 🎊 总结

### ✅ 已实现功能

1. ✅ **自动化爬取** - 无需手动点击
2. ✅ **定时执行** - 每小时自动运行
3. ✅ **日志记录** - 完整执行日志
4. ✅ **失败重试** - 自动重新执行
5. ✅ **资源优化** - 低CPU和内存占用

### 📊 系统状态

- **主服务**：✅ 运行中
- **定时任务**：✅ 已配置
- **公网访问**：✅ 可用
- **数据库**：✅ 正常
- **成本**：**$0/月**

---

## 🌟 最终访问方式

### 公网地址
```
https://3000-i6owb9pva7rgt0fl8drog-5c13a017.sandbox.novita.ai
```

### 管理命令
```bash
cd /home/user/webapp

# 查看服务状态
pm2 list

# 查看爬取日志
tail -f logs/auto_crawler.log

# 手动触发爬取
node auto_crawler.cjs
```

---

**🎉 恭喜！您的风险预警平台已实现完全自动化运行！**

**无需任何手动操作，系统每小时自动更新数据！** 🚀

---

**配置时间**：2026-01-04 06:21  
**执行频率**：每小时1次  
**下次执行**：下一个小时的00:00  
**状态**：✅ 正常运行
